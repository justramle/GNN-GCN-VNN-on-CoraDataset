{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, x, y_true, y_pred):\n",
    "    # Example gradient calculations; specifics depend on your network architecture\n",
    "    # This pseudocode assumes a very simple network and loss for illustration\n",
    "    \n",
    "    # Calculate gradient of loss w.r.t. output of the network (dL/dy_pred)\n",
    "    # Adjust this based on your actual loss calculation\n",
    "    error = y_pred - y_true\n",
    "    \n",
    "    # Backpropagate through output layer\n",
    "    dZ2 = error  # For simplicity, assuming final layer's derivative\n",
    "    dW2 = np.dot(self.a1.T, dZ2) / len(y_true)  # a1 is activation from first layer\n",
    "    dB2 = np.sum(dZ2, axis=0, keepdims=True) / len(y_true)\n",
    "    \n",
    "    # Backpropagate through first layer\n",
    "    dA1 = np.dot(dZ2, self.weights2.T)\n",
    "    dZ1 = dA1 * self.a1 * (1 - self.a1)  # Derivative of sigmoid\n",
    "    dW1 = np.dot(x.T, dZ1) / len(y_true)\n",
    "    dB1 = np.sum(dZ1, axis=0, keepdims=True) / len(y_true)\n",
    "    \n",
    "    return dW1, dB1, dW2, dB2\n",
    "\n",
    "\n",
    "def fit(self, x_train, y_train, x_val, y_val, epochs=100, lr=0.01):\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass on training data\n",
    "        y_pred_train = self.forward(x_train)\n",
    "        loss = self.calculate_cross_entropy_loss(y_pred_train, y_train)\n",
    "        \n",
    "        # Backpropagation to get gradients\n",
    "        grad_w1, grad_b1, grad_w2, grad_b2 = self.backward(x_train, y_train, y_pred_train)\n",
    "        \n",
    "        # Update weights and biases for both layers\n",
    "        self.weights1 -= lr * grad_w1\n",
    "        self.bias1 -= lr * grad_b1\n",
    "        self.weights2 -= lr * grad_w2\n",
    "        self.bias2 -= lr * grad_b2\n",
    "        \n",
    "        # Similar steps for validation as you've written, adjusting as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiLayerNetwork:\n",
    "    def __init__(self, layer_dims):\n",
    "        self.parameters = {}\n",
    "        self.L = len(layer_dims) - 1  # Number of layers excluding input layer\n",
    "        \n",
    "        # Initialize weights and biases for each layer\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.parameters[f'W{l}'] = np.random.randn(layer_dims[l-1], layer_dims[l]) * np.sqrt(2 / layer_dims[l-1])\n",
    "            self.parameters[f'b{l}'] = np.zeros((1, layer_dims[l]))\n",
    "    \n",
    "    def activation_sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def activation_softmax(self, z):\n",
    "        e_x = np.exp(z - np.max(z, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        A = x\n",
    "        for l in range(1, self.L):\n",
    "            Z = np.dot(A, self.parameters[f'W{l}']) + self.parameters[f'b{l}']\n",
    "            A = self.activation_sigmoid(Z)\n",
    "        \n",
    "        # Softmax applied in the last layer\n",
    "        ZL = np.dot(A, self.parameters[f'W{self.L}']) + self.parameters[f'b{self.L}']\n",
    "        AL = self.activation_softmax(ZL)\n",
    "        return AL\n",
    "    \n",
    "    # Dummy backward function for example\n",
    "    def backward(self, x, y_true, y_pred):\n",
    "        # This would be where you implement backpropagation\n",
    "        # For simplicity, we're just returning dummy gradients\n",
    "        return np.ones_like(self.parameters['W1']), np.ones_like(self.parameters['b1'])\n",
    "    \n",
    "    # Adjusted fit method with update mechanism\n",
    "    def fit(self, x_train, y_train, x_val, y_val, epochs=100, lr=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred_train = self.forward(x_train)\n",
    "            loss = self.calculate_cross_entropy_loss(y_pred_train, y_train)\n",
    "            # Update mechanism example for a single layer\n",
    "            grad_weight, grad_bias = self.backward(x_train, y_train, y_pred_train)\n",
    "            \n",
    "            for l in range(1, self.L + 1):\n",
    "                self.parameters[f'W{l}'] -= lr * grad_weight  # Dummy update\n",
    "                self.parameters[f'b{l}'] -= lr * grad_bias    # Dummy update\n",
    "            \n",
    "            # The accuracy calculation and printing logic remains the same...\n",
    "    \n",
    "    def calculate_cross_entropy_loss(self, y_pred, y_true):\n",
    "        y_pred = np.clip(y_pred, 1e-10, 1-1e-10)\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / len(y_true)\n",
    "        return loss\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
